{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "- Daniel Fichthorn: \n",
    "- Evan Asti:  \n",
    "- Saidazim Saidov: \n",
    "- Mulan Zhou: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the relationship between opioids incedance (overdoses, hospitalizations, check into rehab) in Portland and housing prices as well as average time spent on zillow before being sold change in 2020 to 2021?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic is on housing prices & drug usage in Portland, specifically before and after the 2020 Oregon Ballot Measure 110 was passed. Measure 110 reduced the penalties for drug possession for small amounts of controlled substances, reducing it from a class A misdemeanor to class E. We want to study how this may have affected the general population of Portland's access to housing, and if drug usage was related during this time period. By 2024 measure 110 was largely repealed.<a name = \"Drug_Decriminalization\">[</a><sup>4</sup>](#Drug_Decriminalization_ref)\n",
    "\n",
    "A number of studies were conducted about this time period, all which studied the effect of drug decriminalization in Portland in specific categories. \n",
    "\n",
    "One study found that housing instability and opioid related harms often rise together, especially in areas facing rapid policy shifts. Using cross-sectional survey data from unstably housed individuals with a history of drug use, the study identified key demographic and behavioral factors associated with access to housing assistance, including frequency of opioid use and community supervision status. The study stated that, \"Participants were recruited [...] with partner agencies that provide supportive services [...] direct outreach by the study team to homeless\"<a name=\"cite_ref-1\"></a>[<sup>2</sup>](#cite_note-1). While this research focuses on individuals already experiencing housing instability, our study differs by examining broader population-level trends in Portland before and after the passage of Measure 110. Rather than relying on survey data, we analyze changes in housing prices alongside drug usage indicators to explore whether decriminalization coincided with shifts in housing accessibility and market conditions. This allows us to investigate potential associations at a city-wide level and assess how policy changes may have impacted both housing dynamics and drug use simultaneously over time.\n",
    "\n",
    "Another study examined whether Oregon’s 2021 decriminalization of drug possession was linked to changes in overdose mortality while accounting for the spread of fentanyl in the unregulated drug market. The study found that “decriminalization of drug possession was not associated with an increase in fatal drug overdose rates in Oregon in the 2 years after its enactment” once fentanyl was considered as a confounding factor<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2). The authors emphasized that fentanyl is “the principal driver of the overdose mortality epidemic in the US,” suggesting that increases in overdose deaths cannot be attributed to Measure 110 alone. While this research focuses on overdose mortality rather than housing outcomes, it is still relevant to our study because it highlights the importance of accounting for external factors when evaluating the effects of drug policy changes. Our research builds on this idea by examining housing prices and drug usage trends in Portland before and after Measure 110 to better understand how policy changes may have impacted the broader community.\n",
    "\n",
    "How does the relationship between opioids incedance (overdoses, hospitalizations, check into rehab) in Portland and housing prices as well as average time spent on zillow before being sold change in 2020 to 2021.\n",
    "\n",
    "\n",
    "A DID design with time fixed effects from researchers at The Peoples Republic of China finds that measure 110 lowered residential real estate value by more than 1%.<a name = \"110_Econ_article\">[</a><sup>3</sup>](#110_Econ_article_ref) Furthermore, they find that the effects are statsisticly significant, even at the 1% value. The research design for this seems relativly robust, with the authors showing other methods that give similar results. This gives us hope that our analysis will find some interesting correlational patterns, and leads directly into our hypothesis.\n",
    "\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Chung, Esther O et al. “Housing assistance among people who are unstably housed and use drugs in Oregon: a cross-sectional study.” BMC public health vol. 25,1 740. 23 Feb. 2025, doi:10.1186/s12889-025-21925-y.\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Zoorob, Michael J et al. “Drug Decriminalization, Fentanyl, and Fatal Overdoses in Oregon.” JAMA network open vol. 7,9 e2431612. 3 Sep. 2024, doi:10.1001/jamanetworkopen.2024.31612.\n",
    "3. <a name=\"110_Econ_article\"></a>[^](#110_Econ_article_ref) M Joukov, Artem. \"Can You Take Me Higher? No. 110, Drug Decriminalization, and Residential Real Estate Prices.\" Drug Decriminalization, and Residential Real Estate Prices (October 28, 2024) (2024).\n",
    "4. <a name=\"Drug_Decriminalization\"></a>[^](#Drug_Decriminalization_ref) Staudt, Sarah. “Oregon Shouldn’t Go Backwards on Drug Decriminalization.” Www.prisonpolicy.org, 15 Feb. 2024, www.prisonpolicy.org/blog/2024/02/15/oregon-110/.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that there will be a negative correlation between opioid-related incidents and both median housing prices and market velocity (average time spent on Zillow) in Portland between 2020 and 2021. Specifically, as opioid overdoses and hospitalizations increased in certain zip codes, we predict that home sale prices in those areas decreased relative to the city average, while the \"days on Zillow\" significantly increased.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "#%pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    {\n",
    "        'url': 'https://raw.githubusercontent.com/COGS108/Group008_WI26/master/data/00-raw/annual_by_county_death-2026-01-08.csv', 'filename': 'annual_by_county_death-2026-01-08.csv'\n",
    "    }\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight into Measure 110's Before and After Effects on the Housing Market\n",
    "\n",
    "The first dataset we inspect is our attempt at analyzing trends between before Measure 110's enactment, pre Feburary 2021, and post Measure 110's enactment in Portland, Oregon. The dataset is definitely bogged by many unnecessary columns that aren't entirely apparent on the purpose they serve, so we take good caution to omit these columns and only include the ones we feel necessary to our EDA. On the contrary, there are also many in depth columns the Zillow API provides, allowing us to investigate more subtle trends.\n",
    "\n",
    "Given our dataset is for all of Oregon, our most critical metric for filtering our data is the address/city column. This column gives us where the house sold was located, and we will use it to filter our data to particularly fit to Portland. Once this data is filtered, we might consider omitting the address/city column entirely, since we definitively know every house is in Portland. What is mort important is where the house, and the features associated with it; thus, it is important to keep the abbreviatedAddress, # bathrooms, # bedrooms, living area, which is measured in square footage and could be that higher square footage homes indicate higher class area, thereby less drug incedances in that region of Portland, and description columns for each house. All of these provide us some insight in housing prices/time spent on zillow. We would want to consider these elements and give opertational definitions on the effect they had on a housing price, if notable at all. We may revisit this in relation to if these opioid incedances, such as did houses which were considered in more \"clean\" parts of the neighborhood skyrocket in price compared to less clean? Also, were these houses bought up faster in response to other houses? How would we measure that? How would we want to consider an outside sources/data as to if the house was in a more affulent/clean area of Portland?\n",
    "\n",
    "In regard to price skyrocket inquiry, Zillow API features columns which allow us to perform EDA on the data trends. We will use the columns labeled as \"priceHistory/*number*/*feature*\". These columns give information on a price change occuring for a particular house, as well as various features associated with that price change, like price, event, date, and price change rate. Namely, this column set is one of the only that extends outside of our data range, which is something we will want to account for. Note that although filtering this data is important, it appears some of it is out of our range of coding knowledge, so I will be looking into documentation to filter. For now, we keep empty cells as \"N/A\", and filled cells as they are. For data that does lie in our range, we plan to use more of the features of priceHistory, and investigate more trends; for now, we do not do this, since the data is not entirely ready for that use. Are certain houses in the range we are interested in for our dataset, Jul 2020-Jul 2021, having more price history changes? If so, what other columns could we combine to figure out why this change might be occuring? We can note in particular that there are more changes that are outside our desired range than inside our desired range, which does limit our data, but we will utilize the data that we do have to create effective conclusions.\n",
    "\n",
    "Because of Zillow's relatively frequent updates, our data set is relatively tidy. We will set the \"priceHistory/*number*/*feature*\" to be \"N/A\" for columns which do not have any updates in our desired range. We want to make sure we tidy our dataset such that we can create new columns which associate to our other dataset on incedances. Thereby, we also want to insert outlier columns during our EDA, but for now, we need to tidy the dataset on aspects like housing prices, potentially considering an approach where we groupby certain price ranges. Another way to group is the daysOnZillow category, and we will definitely want to combine this with our priceHistory categories. This way, we could see how many times there were changes in the window which the house was for sale and sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_417/1869222221.py:4: DtypeWarning: Columns (20,50,161,200) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  oregon_housing_df = pd.read_csv('portland_housing.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25731, 348)\n",
      "abbreviatedAddress      0\n",
      "address/city            0\n",
      "address/zipcode         0\n",
      "bathrooms             207\n",
      "bedrooms              448\n",
      "description            43\n",
      "livingArea            196\n",
      "dtype: int64\n",
      "livingArea            196\n",
      "bedrooms              193\n",
      "bathrooms             191\n",
      "abbreviatedAddress      0\n",
      "address/city            0\n",
      "address/zipcode         0\n",
      "description             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "oregon_housing_df = pd.read_csv('portland_housing.csv')\n",
    "print(oregon_housing_df.shape)\n",
    "portland_housing_df = oregon_housing_df[oregon_housing_df['address/city'] == 'Portland']\n",
    "portland_housing_df = portland_housing_df[['abbreviatedAddress','address/city','address/zipcode','bathrooms','bedrooms','description', 'livingArea']]\n",
    "# Check for missing data\n",
    "portland_housing_missing_df = portland_housing_df.isna().sum()\n",
    "print(portland_housing_missing_df)\n",
    "# We can see that the missing data for bathrooms, bedrooms, description, and livingArea are relatively low compared to the size of the dataSet\n",
    "# For bathrooms, bedrooms, livingArea we classify this as \"Unknown\"\n",
    "# For description, we classify this as \"N/A\"\n",
    "# First, check to see if livingArea N/A are correlated with bathroom and bedrooms N/A\n",
    "living_area = portland_housing_df['livingArea'].isna()\n",
    "print(portland_housing_df.loc[living_area].isna().sum().sort_values(ascending=False))\n",
    "# Near 100% relationship, so we will consider dropping these columns later\n",
    "\n",
    "# Convert to dateTime objects\n",
    "oregon_housing_df[\"dateSold\"] = pd.to_datetime(oregon_housing_df[\"dateSold\"], unit=\"ms\")\n",
    "priceHistory_df = oregon_housing_df[['daysOnZillow', 'dateSold', 'priceHistory/0/date','priceHistory/1/date', 'priceHistory/2/date', 'priceHistory/3/date',\n",
    "'priceHistory/4/date', 'priceHistory/5/date', 'priceHistory/6/date', 'priceHistory/7/date', 'priceHistory/8/date',\n",
    "'priceHistory/9/date']]\n",
    "priceHistory_df = priceHistory_df.fillna(\"N/A\")\n",
    "# Should always be the case that priceHistory/0/date is the same as dateSold, so we can drop this later\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oregon Overdose Death Data\n",
    "\n",
    "https://oregoninjurydata.shinyapps.io/overdose/#county3\n",
    "\n",
    "This dataset is rather straigntforward. It has information for opiod related deaths by county for all of Oregon from 210 to 2024. This data is already rather clean, so the only real thing to do here is to change the columns from strings into a more appropriate type so that we can use our analytic tools on them. Here the count collumn indicates the amount of people who died due to opioid related incidents for the corresponding county and year. The rate column shows the amount of deaths per one hundred thousand people in the county.\n",
    "\n",
    "The only possible concern we can see with this data is that this is that according to the Oregon Health Authority \"Not all deaths are certified by a medical examiner in Oregon, therefore not all deaths will have a toxicology test performed.\" This means that this data likely undrepresents the true amount of opioid related deaths in the state. There is (to our knowledge) no way to account for this properly, so we will have to keep that in mind when we begin our more formal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year               0\n",
       "County             0\n",
       "Data Type          0\n",
       "Drug Type          0\n",
       "Count              0\n",
       "Rate (per 100K)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the usual packages and load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "oregon_deaths_df = pd.read_csv('data/00-raw/annual_by_county_death-2026-01-08.csv')\n",
    "\n",
    "#Lets also check for missing values\n",
    "oregon_deaths_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>County</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Drug Type</th>\n",
       "      <th>Count</th>\n",
       "      <th>Rate (per 100K)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>Multnomah</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>412</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>Clatsop</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>10</td>\n",
       "      <td>24.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>12</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>Umatilla</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>8</td>\n",
       "      <td>9.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wallowa</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>Morrow</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>Count between 1 and 4.</td>\n",
       "      <td>Rate not reportable. Count between 1 and 4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>Union</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>Count between 1 and 4.</td>\n",
       "      <td>Rate not reportable. Count between 1 and 4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>Gilliam</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>76</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>Sherman</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year      County Data Type   Drug Type                   Count  \\\n",
       "0  2024   Multnomah    Deaths  Any Opioid                     412   \n",
       "1  2024     Clatsop    Deaths  Any Opioid                      10   \n",
       "2  2024    Columbia    Deaths  Any Opioid                      12   \n",
       "3  2024    Umatilla    Deaths  Any Opioid                       8   \n",
       "4  2024     Wallowa    Deaths  Any Opioid                       0   \n",
       "5  2024      Morrow    Deaths  Any Opioid  Count between 1 and 4.   \n",
       "6  2024       Union    Deaths  Any Opioid  Count between 1 and 4.   \n",
       "7  2024     Gilliam    Deaths  Any Opioid                       0   \n",
       "8  2024  Washington    Deaths  Any Opioid                      76   \n",
       "9  2024     Sherman    Deaths  Any Opioid                       0   \n",
       "\n",
       "                               Rate (per 100K)  \n",
       "0                                        51.77  \n",
       "1                                        24.36  \n",
       "2                                         22.2  \n",
       "3                                         9.94  \n",
       "4                                            0  \n",
       "5  Rate not reportable. Count between 1 and 4.  \n",
       "6  Rate not reportable. Count between 1 and 4.  \n",
       "7                                            0  \n",
       "8                                        12.43  \n",
       "9                                            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look at the data and check for missing values\n",
    "oregon_deaths_df.head(10)\n",
    "\n",
    "# We see that we will need to map Count and Rate to Numerical objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                 int64\n",
       "County              object\n",
       "Data Type           object\n",
       "Drug Type           object\n",
       "Count                int64\n",
       "Rate (per 100K)    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will map the low number counts 'Counts between 1 and 4' to its average rounded down (2) , and just set the rate to missing for now. We will drop these later but we believe we may want those rows for analysis later\n",
    "oregon_deaths_df = oregon_deaths_df.replace({\n",
    "    'Count between 1 and 4.': 2,\n",
    "    'Rate not reportable. Count between 1 and 4.': np.nan\n",
    "})\n",
    "\n",
    "#Now make Count and Rate into numeric variables\n",
    "oregon_deaths_df['Count'] = pd.to_numeric(oregon_deaths_df['Count'])\n",
    "oregon_deaths_df['Rate (per 100K)'] = pd.to_numeric(oregon_deaths_df['Rate (per 100K)'])\n",
    "\n",
    "#Then Check the dtypes\n",
    "oregon_deaths_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>County</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Drug Type</th>\n",
       "      <th>Count</th>\n",
       "      <th>Rate (per 100K)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>Multnomah</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>412</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>Clatsop</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>10</td>\n",
       "      <td>24.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>12</td>\n",
       "      <td>22.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>Umatilla</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>8</td>\n",
       "      <td>9.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wallowa</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>Morrow</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>Union</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>Gilliam</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>76</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>Sherman</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>Any Opioid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year      County Data Type   Drug Type  Count  Rate (per 100K)\n",
       "0  2024   Multnomah    Deaths  Any Opioid    412            51.77\n",
       "1  2024     Clatsop    Deaths  Any Opioid     10            24.36\n",
       "2  2024    Columbia    Deaths  Any Opioid     12            22.20\n",
       "3  2024    Umatilla    Deaths  Any Opioid      8             9.94\n",
       "4  2024     Wallowa    Deaths  Any Opioid      0             0.00\n",
       "5  2024      Morrow    Deaths  Any Opioid      2              NaN\n",
       "6  2024       Union    Deaths  Any Opioid      2              NaN\n",
       "7  2024     Gilliam    Deaths  Any Opioid      0             0.00\n",
       "8  2024  Washington    Deaths  Any Opioid     76            12.43\n",
       "9  2024     Sherman    Deaths  Any Opioid      0             0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally lets investigate the data once again\n",
    "oregon_deaths_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Rate (per 100K)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>377.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.731481</td>\n",
       "      <td>10.217347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.379465</td>\n",
       "      <td>10.777501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>532.000000</td>\n",
       "      <td>66.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Count  Rate (per 100K)\n",
       "count  540.000000       377.000000\n",
       "mean    14.731481        10.217347\n",
       "std     39.379465        10.777501\n",
       "min      0.000000         0.000000\n",
       "25%      2.000000         0.000000\n",
       "50%      2.000000         8.130000\n",
       "75%     12.000000        15.470000\n",
       "max    532.000000        66.980000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looks allright to me - lets move on to check for outliers and some basic descriptive statistics\n",
    "oregon_deaths_df[['Count', 'Rate (per 100K)']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see at least one clear outlier from our head call 2 cells above - lets check up on that entry. A quick google search seems to confirm that these numbers are accurate. As the max of count is remarkably similar (same order of magnitude) we are inclined to believe that all of this data makes sense and the high values are abnormal but not due to human error. Here is an article from 2024 that supports this position: https://multco.us/news/multnomah-county-releases-2023-domicile-unknown-report-homeless-deaths.\n",
    "Note that the standard deviations reported here are incorrect as these calculations are not clustered properly, so do not be alarmed by its large value. The other statistics should all be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CSV with missing values under interim \n",
    "oregon_deaths_df.to_csv('data/01-interim/annual_by_county_death_nas.csv', index=False)\n",
    "\n",
    "# make a copy where we drop the missing value rows completely and save under processed\n",
    "oregon_deaths_df_clean = oregon_deaths_df.dropna(subset=['Rate (per 100K)'])\n",
    "oregon_deaths_df_clean.to_csv('data/02-processed/annual_by_county_death_clean.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Different Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      county zip_code\n",
      "0  Multnomah    97010\n",
      "1  Multnomah    97019\n",
      "2  Multnomah    97024\n",
      "3  Multnomah    97030\n",
      "4  Multnomah    97060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = {\n",
    "    \"Multnomah\": \"97010, 97019, 97024, 97030, 97060, 97080, 97201, 97202, 97203, 97204, 97205, 97206, 97209, 97210, 97211, 97212, 97213, 97214, 97215, 97216, 97217, 97218, 97219, 97220, 97221, 97227, 97230, 97231, 97232, 97233, 97236, 97239, 97266\",\n",
    "    \"Washington\": \"97005, 97006, 97007, 97008, 97062, 97075, 97076, 97077, 97106, 97113, 97116, 97119, 97123, 97124, 97125, 97133, 97140, 97223, 97224, 97225, 97229\",\n",
    "    \"Clatsop\": \"97102, 97103, 97110, 97121, 97138, 97145, 97146\",\n",
    "    \"Columbia\": \"97018, 97048, 97051, 97053, 97054, 97056, 97064\",\n",
    "    \"Umatilla\": \"97801, 97810, 97813, 97826, 97835, 97838, 97859, 97862, 97868, 97875, 97880, 97882, 97886\",\n",
    "    \"Wallowa\": \"97828, 97842, 97846, 97885\",\n",
    "    \"Morrow\": \"97818, 97836, 97839, 97843, 97844\",\n",
    "    \"Union\": \"97824, 97827, 97850, 97867, 97876, 97883\",\n",
    "    \"Gilliam\": \"97001, 97012, 97812, 97830\",\n",
    "    \"Sherman\": \"97033, 97039, 97050, 97065\"\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for county, zips in raw_data.items():\n",
    "    for zipcode in zips.split(\", \"):\n",
    "        rows.append({\"county\": county, \"zip_code\": zipcode})\n",
    "\n",
    "df_crosswalk = pd.DataFrame(rows)\n",
    "\n",
    "print(df_crosswalk.head())\n",
    "\n",
    "df_crosswalk.to_csv(\"county_zip_crosswalk.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Zillow housing data are categorized by Zip Code and the opioid health data is reported at the County level, a direct join is impossible for us. We will resolve this by mapping each of the 10 Oregon counties to their respective Zip Codes. This allows us to perform a many to one join, so that each housing neighborhood can be matched with its corresponding regional health statistic. We will need to ensure the zip codes are formatted as strings to avoid losing any of the leading zeroes (if there are). Afterwards, we will merge the datasets using the County name as a common key. While we haven't exactly decided how to interpret the data, one potential use for this is to find a baseline mean. By subtracting this baseline from each zip code's performance, we can derive a value to correlate against the opioid incident rates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> Example of how to use the checkbox, and also of how you can put in a short paragraph that discusses the way this checklist item affects your project.  Remove this paragraph and the X in the checkbox before you fill this out for your project\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    ">Drug incidence data is often a proxy for policing intensity rather than actual drug usage rates. Low-income neighborhoods or areas with more public transit may show higher drug incidence in police records because of increased surveillance, not necessarily because drug use is higher than high-income areas.\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    ">We should ensure that you are using aggregated data rather than individual data.\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    ">Our project involves \"protected group status\" implicitly through geographic proxies (zip codes). There is a risk that findings could be used to further stigmatize specific Portland neighborhoods or justify \"redlining-like\" behavior in real estate. We should attempt to discuss the socioeconomic confounding variables (like poverty rates or historical disinvestment) that impact both housing prices and public health outcomes.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    ">While the datasets are public, we will maintain a secure workflow to ensure data security. All data will be processed and stored in a private Github repository. Only team members and course staff will have access.\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    ">Our team will retain the data only for the duration of the Winter 2026 quarter. Once we have received our final grade, we will delete all local copies of the dataset and archive the Github repository.\n",
    "\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    ">We are aware of potential confounds related to race, financial background, and mental illness. All of these are well documented elements in Measure 110's history, and we believe that the model results will be generalized to be fair with respect to different groups, since we will not be concentrating on any of these aspects in our models. Groups could be revisited in our conclusion if a weak correlation is drawn.\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    ">Additional metrics plan to be substantiated by the background given, and we have also uncovered papers which have good data models that we want to replicate. By replicating these models, we can wrangle the data to work with other datasets of ours, and begin to explore and analyze data further. Optimizing our defined metrics directly is related to consulting outside sources, and we are prepared to cite these sources appropriately.\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    ">Similar to D.2, if we find that our model accentuates more complex relationships in race for instance, we would want a disclaimer of what our data looks at and the connection it aims to establish, and what the data does not suggest. We want to ensure our data is communicated in a way that is indicative of what we wanted to establish a correlational between, and do not want our data used in samples that to make correlations that it was not intended for. If users are excessively harmed, we will act accordingly to have our data be more selective to public. This also doubles as an answer for E.4.\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Timely Communication and do so in a respectful manner\n",
    "* Be Ethical with all our work & conversations\n",
    "* Respectable to everyone in the group "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/18  | 6 PM  | Clean data; Import & Wrangle Data (Portland Drug Usage & Housing in Portland) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/25  | 6 PM  | Finalize wrangling/EDA; Begin Analysis | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 6 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project; Finalize |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
